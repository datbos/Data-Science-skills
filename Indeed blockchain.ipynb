{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import urllib2 # Website connections\n",
    "import re # Regular expressions\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create website content cleaning function to isolate non stop words for key word counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning for the raw html is necessary to get the final terms we are looking for. Extract the relevant portions of the html, get the text, removes blank lines and line endings, removes unicode, and filters with regular expressions to include only words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleaner(website):\n",
    "    '''\n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        site = urllib2.urlopen(website).read() # Connect to the job posting\n",
    "    except: \n",
    "        return   # Need this in case the website isn't there anymore or some other weird connection problem \n",
    "    soup_obj = BeautifulSoup(site, \"lxml\") # Get the html from the site\n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "        text = soup_obj.get_text() # Get the text from this\n",
    "        lines = (line.strip() for line in text.splitlines()) # break into lines\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    def chunk_space(chunk):\n",
    "        chunk_out = chunk + ' ' # Need to fix spacing issue\n",
    "        return chunk_out  \n",
    "    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends \n",
    "    #of line\n",
    "    # Now clean out all of the unicode junk (this line works great!!!)\n",
    "    try:\n",
    "        text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted\n",
    "    except:                                                            # in a way that this works, can occasionally throw\n",
    "        return                                                         # an exception\n",
    "    text = re.sub(\"[^a-zA-Z.+3]\",\" \", text)  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "    # Also include + for C++\n",
    "    text = text.lower().split()  # Go to lower case and split them apart\n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "    text = list(set(text)) # Last, just get the set of these. Ignore counts (we are just looking at whether a term existed\n",
    "    # or not on the website)\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for specified city, state and job title indeed website URL list development "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def city_state(city = None, state = None):\n",
    "    ''' This function accepts the desired city and state for Indeed search and returns the\n",
    "    web site url. multi word cities are accoodated for like Salt Lake City'''\n",
    "    final_job = 'Hyperledger%2C+Ethereum%2C+Stellar%2C+Ripple%2C+Eris%2C+Corda'\n",
    "    # searching for data scientist exact fit(\"data scientist\" on Indeed search)\n",
    "    # Make sure the city specified works properly if it has more than one word(such as San Francisco)\n",
    "    if city is not None:\n",
    "        final_city = city.split()\n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=', final_job, '&l=', final_city,'%2C+', state]\n",
    "        # Join all of our strings together so that indeed will search correctly\n",
    "    else:\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "    \n",
    "    final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "    return final_site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_jobs(final_site,city):\n",
    "    base_url = 'http://www.indeed.com'\n",
    "\n",
    "    try:\n",
    "        html = urllib2.urlopen(final_site).read() # Open up the front page of our search first\n",
    "    except:\n",
    "        'That city/state combination did not have any jobs. Exiting . . .' # In case the city is invalid\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"lxml\") # Get the html from the first page\n",
    "    # Now find out how many jobs there were\n",
    "    num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8')    # Now extract the total number of jobs found\n",
    "                                                                            # The 'searchCount' object has this\n",
    "    job_numbers = re.findall('\\d+', num_jobs_area) # Extract the total jobs found from the search result\n",
    "    print (job_numbers)\n",
    "    if len(job_numbers) > 3:\n",
    "        #Have a total number of jobs greater than 1000\n",
    "        total_num_jobs = (int(job_numbers[1]) * 1000) + int(job_numbers[2])\n",
    "    else:\n",
    "        total_num_jobs = int(job_numbers[1])\n",
    "    \n",
    "    city_title = city\n",
    "    if city is None:\n",
    "        city_title = 'Nationwide'\n",
    "    \n",
    "    print 'There were', total_num_jobs, 'jobs found,', city_title  # Display how many jobs were found\n",
    "    num_pages = total_num_jobs / 10   # This will be how we know the number of times we need to iterate \n",
    "#    print num_pages, \"pages\"                                  # over each new search result page\n",
    "    return num_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.indeed.com/jobs?q=Hyperledger%2C+Ethereum%2C+Stellar%2C+Ripple%2C+Eris%2C+Corda&l=new+york%2C+ny\n",
      "['1', '683']\n",
      "There were 683 jobs found, new york\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "x = city_state('new york','ny')\n",
    "print x\n",
    "y = number_jobs('http://www.indeed.com/jobs?q=Hyperledger%2C+Ethereum%2C+Stellar%2C+Ripple%2C+Eris%2C+Corda&l=new+york%2C+ny','new york')\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_description(job_URLS):\n",
    "    \n",
    "    job_descriptions = []\n",
    "    final_description = []\n",
    "    pool = Pool(15)\n",
    "    job_descriptions = pool.map(text_cleaner, job_URLS)\n",
    "    print final_description\n",
    "\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "  #  for j in xrange(0, len(job_URLS)):\n",
    "  #       print len(job_URLS)-j,\n",
    "  #      final_description = text_cleaner(job_URLS[j])\n",
    "  #      if final_description: #So that we only append when the website was accessed correctly\n",
    "  #          job_descriptions.append(final_description)\n",
    "#        sleep(1)# So that we don't be jerks. If you have a very fast internet connection you could hit the server a lot! \n",
    "\n",
    "    return job_descriptions\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_jobs(num_pages, final_site):\n",
    "    job_descriptions = []             # Store all our descriptions in this list\n",
    "    job_URLS = []\n",
    "    base_url = 'http://www.indeed.com'\n",
    "    print 'Getting page',\n",
    "    for i in xrange(1, num_pages + 1): #Loop through all of our search result pages\n",
    "        print i,\n",
    "        start_num = str(i * 10)       # Assign the multiplier of 10 to view the pages we want\n",
    "        current_page = ''.join([final_site, '&start=', start_num])\n",
    "       \n",
    "        # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "        html_page = urllib2.urlopen(current_page).read() # Get the page        \n",
    "        page_obj = BeautifulSoup(html_page, \"lxml\")      # Locate all of the job links\n",
    "\n",
    "        job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "        for link in job_link_area.find_all('a'):\n",
    "            if link.get('href') != None:                \n",
    "                job_URLS.append(base_url + link.get('href'))\n",
    "\n",
    "                #job_URLS = [base_url + link.get('href') for link in job_link_area.find_all('a')]\n",
    "                # Get the URLS for the jobs\n",
    "        job_URLS = filter(lambda x: 'clk' in x, job_URLS) # Now get just the job related URLS\n",
    "\n",
    "    return job_URLS    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skills_info(city = None, state = None):\n",
    "    '''\n",
    "    This function will take a desired city / state and look\n",
    "    for all new job postings on Indeed.com. It will crawl all of the job postings and keep track of how many\n",
    "    use a preset list of typical data science skills. The final percentage\n",
    "    for each skill is then displayed at the end of the collation.\n",
    "    Inputs: The location 's city and state. These are optional. If no city/state is input, \n",
    "    the function will assume a national search(this can take a while !!!).\n",
    "    Input the city / state as strings, such as skills_info('Chicago', 'IL').\n",
    "    Use a two letter abbreviation for the state.\n",
    "    Output: A bar chart showing the most commonly desired skills in the job market for\n",
    "    a data scientist.\n",
    "    '''\n",
    "\n",
    "    #final_site = city_state(city, state)\n",
    "    sample = city_state(city,state)\n",
    "    \n",
    "    sample_no = number_jobs(sample,city)\n",
    "    \n",
    "    \n",
    "    sample_jobs = find_jobs(sample_no,sample)\n",
    "    \n",
    "               \n",
    "    job_descript = get_description(sample_jobs)\n",
    "    \n",
    "    print 'Done with collecting the job postings!'\n",
    "    print 'There were', len(job_descript), 'jobs successfully found.'\n",
    "    \n",
    "    city_title = city\n",
    "    doc_frequency = Counter()  # This will create a full counter of our terms.\n",
    "    [doc_frequency.update(item) for item in job_descript]  # List comp\n",
    "    # Now we can just look at our final dict list inside doc_frequency\n",
    "\n",
    "    # Obtain our key terms and store them in a dict. \n",
    "    # These are the key data science skills we are looking for\n",
    "    prog_lang_dict = Counter({\n",
    "        'C': doc_frequency['C'],\n",
    "        'C#': doc_frequency['C#'],\n",
    "        'node.js': doc_frequency['node.js'],\n",
    "        'Python': doc_frequency['python'],\n",
    "        'Java': doc_frequency['java'],\n",
    "        'C++': doc_frequency['c++'],\n",
    "        'cuda': doc_frequency['cuda'],\n",
    "        'Perl': doc_frequency['perl'],\n",
    "        'openCL': doc_frequency['openCL'],\n",
    "        'JavaScript': doc_frequency['javascript'],\n",
    "        'Solidity': doc_frequency['solidity'],\n",
    "        'swarm': doc_frequency['swarm'],\n",
    "        'whisper': doc_frequency['whisper']\n",
    "    })\n",
    "    analysis_tool_dict = Counter({\n",
    "        'geth': doc_frequency['geth'],\n",
    "        'parity': doc_frequency['parity'],\n",
    "        'web3.js': doc_frequency['web3.js'],\n",
    "        'metamask': doc_frequency['metamask'],\n",
    "        'truffle': doc_frequency['truffle'],\n",
    "        'testrpc': doc_frequency['testrpc']\n",
    "    })\n",
    "    hadoop_dict = Counter({\n",
    "        'solc': doc_frequency['solc'],\n",
    "        'solium': doc_frequency['solium'],\n",
    "        'ether.camp': doc_frequency['ether.camp'],\n",
    "        'blockapps': doc_frequency['blockapps'],\n",
    "        'embark': doc_frequency['embark'],\n",
    "        'zeppelin': doc_frequency['zeppelin'],\n",
    "        'embark': doc_frequency['embark'],\n",
    "        'baas': doc_frequency['baas'],\n",
    "        'tierion': doc_frequency['tieron'],\n",
    "        'ether scripter': doc_frequency['ether scripter']\n",
    "    })\n",
    "    database_dict = Counter({\n",
    "        'SQL': doc_frequency['sql'],\n",
    "        'NoSQL': doc_frequency['nosql'],\n",
    "        'HBase': doc_frequency['hbase'],\n",
    "        'Cassandra': doc_frequency['cassandra'],\n",
    "        'MongoDB': doc_frequency['mongodb'],\n",
    "        'mining': doc_frequency['mining'],\n",
    "        'phd': doc_frequency['phd'],\n",
    "        'block chain': doc_frequency['block chain']\n",
    "    })\n",
    "    overall_total_skills = prog_lang_dict + analysis_tool_dict + hadoop_dict + database_dict\n",
    "    # Combine our Counter objects\n",
    "    print overall_total_skills\n",
    "    final_frame = pd.DataFrame(overall_total_skills.items(), columns = ['Term', 'NumPostings'])\n",
    "    # Convert these terms to a dataframe\n",
    "    # Change the values to reflect a percentage of the postings\n",
    "    final_frame.NumPostings = (final_frame.NumPostings) * 100 / len(job_descript)\n",
    "    # Gives percentage of job postings# having that term\n",
    "    # Sort the data for plotting purposes\n",
    "    #final_frame.sort(key=takeSecond, reverse=True)  # Get it ready for a bar plot\n",
    "    final_frame.sort_values('NumPostings', ascending = False, inplace = True)\n",
    "    \n",
    "    final_plot = final_frame.plot(x = 'Term', kind = 'bar', legend = None, title = 'Percentage of Job Ads with a Key Skill, ' + city_title)\n",
    "    final_plot.set_ylabel('Percentage Appearing in Job Ads')\n",
    "    fig = final_plot.get_figure()     # Have to convert the pandas plot object to a matplotlib object\n",
    "    return fig, final_frame  # End of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.indeed.com/jobs?q=Hyperledger%2C+Ethereum%2C+Stellar%2C+Ripple%2C+Eris%2C+Corda&l=new+york%2C+ny\n",
      "Getting page"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5c8a0c545df5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mseattle_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskills_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'new york'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ny'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mseattle_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-cd8603ec7ba4>\u001b[0m in \u001b[0;36mskills_info\u001b[0;34m(city, state)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0msample_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_no\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-acd0e56acb36>\u001b[0m in \u001b[0;36mfind_jobs\u001b[0;34m(num_pages, final_site)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbase_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://www.indeed.com'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Getting page'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pages\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Loop through all of our search result pages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mstart_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# Assign the multiplier of 10 to view the pages we want\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "\n",
    "seattle_info = skills_info(city = 'new york', state = 'ny') \n",
    "seattle_info\n",
    "\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
